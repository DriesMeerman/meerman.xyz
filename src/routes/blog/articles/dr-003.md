---
title: Hybrid automation for one-off tasks
tags: 
date: 2023-10-04
author: Dries Meerman
ID: 003
summary: The blog post titled "Hybrid Automation" explores a practical approach to combining automation and manual tasks to achieve optimal efficiency. The author introduces the concept of hybrid automation, where automation is applied to tasks that are easily automated, and manual tasks are retained if they are not too tedious and don't benefit significantly from automation. The post delves into the Pareto Principle, commonly known as the 80/20 rule, where 80% of outcomes come from 20% of causes, emphasizing the efficient use of resources. The author provides a real-life example involving the automated downloading of images from a website and explains how they leveraged the ChatGPT tool to simplify the process. They describe the creation of scripts for obtaining image links, copying the data to the clipboard, and a bash script for downloading the images. The post concludes by highlighting the benefits of breaking tasks into manageable components that can be easily automated or manually executed to bridge gaps effectively.
---

![Vaporwave computer graphic generated using Dalle-2](assets/articles/dr-3/vaporwave_computer.png)

# Hybrid Automation
What is it?
I would describe at the combination between using automation when it is easy and performing manual tasks where the manual task is not too tedious and automating does not add much value.
The goal of this is being as efficient as possible with your time, don't waste time spending hours to make a complex script. When you can make a simple script in 20% of the time and a small manual action.

In the rest of this post I will first explain the principle which is why I think hybrid automation works so well.
Then I will continue giving a real life example in which I have used these techniques to partially automate a task for myself.
Finally we'll end with some concluding thoughts.

# The Pareto Principle
Also known as the [80/20 rule](https://en.wikipedia.org/wiki/Pareto_principle) where 80% of the outcomes come from 20% of the causes.
20% of the time, budget, knowledge etc.. is spent on 80% of the result you are trying to achieve.
The final 20% of the outcome will cost you 80% of the effort.
For hybrid automation with 20% of scripting knowledge you will be able to achieve 80% of your goal.
The rest we can do manually.

Additionally for one of tasks we care about the result and not to process to get there.
We should be mindful of spending the effort where it is most valuable.
Some tasks ara easier performed manually where it will be less effort while still getting the 80% result you want.
Other tasks can easily be automated with a small amount of effort leading to the result you want.

Therefore hybrid automation allows you to utilize this principle from two fronts both with manual actions 

>So that's nice in theory and all but what can we actually use this for?

# Downloading Pictures
Recently I went to an event that had a open photo-booth and the pictures taken where shared on a webpage.
Me being a bit of a data-hoarder wanted to download all the pictures so I can later sort them out. As I don't know until when these pictures will stay up on the website.
One way of solving this issue is being a normal person opening the website and right-click saving the images I want to keep.
However in total there where hundreds of pictures and I couldn't be bothered to figure out which ones I want now.
Saving them all manually seemed also like too much of a tedious task.

But the structure of the website was relatively simple it only contained an infinite loading grid of the images.
So I asked ChatGPT[^1] for a simple script I could copy into my browsers console to get the links of all these images.
It gave me a script, I scrolled all the way down the page so all links would be available and ran the script.
Trying to scrape all the image of the page with a script would have been a lot more complex without leveraging the existing websites behaviour E.G. manually scrolling and loading all the image urls.

In total it where some 300 images, so the output of the `console.log` became to large too easily copy. 
Therefore I just asked ChatGPT again to modify the script to pass the data directly into my clipboard.

This resulted in the following script:
```javascript
// Select all img tags on the webpage
const imgTags = document.querySelectorAll('img');

// Initialize an array to store the cleaned src URLs
const cleanedSrcArray = [];

// Loop through each img tag and process its src attribute
imgTags.forEach(img => {
  // Get the src attribute
  let src = img.getAttribute('src');

  // Remove query parameters (if any)
  src = src.split('?')[0];

  // Add the cleaned src to the array
  cleanedSrcArray.push(src);
});

// Convert the array to a string
const cleanedSrcString = cleanedSrcArray.join('\n');

// Copy the string to the clipboard
const clipboardText = document.createElement('textarea');
clipboardText.value = cleanedSrcString;
document.body.appendChild(clipboardText);
clipboardText.select();
document.execCommand('copy');
document.body.removeChild(clipboardText);

// Display a message to confirm that the data has been copied
console.log('Cleaned URLs copied to clipboard:');
console.log(cleanedSrcArray);
```

I then saved the contents of my clipboard into a file. 
Finally I asked ChatGPT for a bash script to download each image found in that file.
Making sure to put some wait times between downloads as to be a good citizen and not overload the server that is hosting these images for no reason.
Then I executed the script and I waited until it downloaded all the images.

```bash
#!/bin/bash

# Ensure photolinks.txt exists and is not empty
if [ ! -s "photolinks.txt" ]; then
  echo "photolinks.txt is empty or does not exist. Please populate it with image URLs."
  exit 1
fi

# Create a directory to store downloaded images (if it doesn't exist)
mkdir -p downloaded_images

# Loop through each line in photolinks.txt and download the images
while IFS= read -r url; do
  # Generate a unique filename based on the URL
  filename=$(basename "$url")

  # Download the image with wget
  wget -O "downloaded_images/$filename" "$url"

  # Wait for 1 second before the next download
  sleep 1
done < photolinks.txt

echo "All images have been downloaded."
```

The whole thing took less than 15 minutes, the amount of time spent scripting about equaled the download time.
But that was in part because I did not try to create a perfect system that did everything in one go.


# Conclusions
By splitting up the problem into multiple small tasks that where either easy to automate or easy to do manually.
I could bridge a gap that would have otherwise been complex to automate or tedious to do manually.
The increase in efficiency lowered the barrier of the task for me, fully scripting or manually downloading the images would have been a bar of effort too high.
Leading to me not having these images in my personal archive and all things being equal I'd rather have them.

# Notes
[^1]: ChatGPT is a generative large language model capable of writing code